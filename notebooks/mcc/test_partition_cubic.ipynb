{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f3efb3-a65c-4325-b8bd-24a70456e29a",
   "metadata": {},
   "source": [
    "# Test partition_cubic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4316d748-1288-473c-b35d-cc41111b9335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "from FACS_Sampling.methods.methods import bin_sample, sample_random #, dist_sampling\n",
    "from FACS_Sampling.utils import create_adata\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# Get the file path from environment variable\n",
    "file_path_env = os.getenv('MY_FACS_DATA_PATH')\n",
    "OBS_FEATURES = ['prediction','organ','sample_group','label','group','celltype','sample_id']\n",
    "DROP_FEATURES = ['SSC-B-H','SSC-B-A']\n",
    "\n",
    "REFERENCES = [1, 5, 10, 20, 34]\n",
    "METHODS = ['random', 'cubic', 'atomic', 'hopper']\n",
    "methods = ['random', 'cubic', 'hopper']\n",
    "SIZES = [50000, 100000, 200000]\n",
    "REPS = [i for i in range(5)]\n",
    "label_key = 'celltype'\n",
    "\n",
    "\n",
    "directory = \"lcmv/benchmark\"\n",
    "PATH = os.path.join(file_path_env, directory)\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def pca_bin_sample_(df, feature_importances, seed=12345):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Ensure num_pcs does not exceed the number of columns in df\n",
    "    num_pcs = min(feature_importances.shape[0], df.shape[1])\n",
    "\n",
    "    # Function to create bins and digitize\n",
    "    def create_bins_and_digitize(data, n_bins):\n",
    "        edges = np.linspace(data.min(), data.max(), n_bins + 1)\n",
    "        bins = np.digitize(data, edges)\n",
    "        return bins\n",
    "\n",
    "    def compute_sample_bins(df, bin_sizes):\n",
    "        bins = [create_bins_and_digitize(df.iloc[:, i], bin_sizes[i]) for i in range(num_pcs)]\n",
    "\n",
    "        # Combine bins to form grid cells\n",
    "        df['grid_cell'] = list(zip(*bins))\n",
    "        \n",
    "        return \n",
    "\n",
    "    compute_sample_bins(df, feature_importances)\n",
    "    return\n",
    "\n",
    "    \n",
    "def set_min_to_two(pca):\n",
    "    out = np.ceil(pca.explained_variance_ratio_*100).astype(int)\n",
    "    return out[out>2]\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "def find_threshold_index(sorted_grid_cells, threshold):\n",
    "    cumulative = 0\n",
    "    for index, frequency in sorted_grid_cells.value_counts().sort_index().items():\n",
    "        cumulative += index * frequency\n",
    "        if cumulative >= threshold:\n",
    "            return index\n",
    "    return None\n",
    "\n",
    "def accumulate_indices_until_threshold(df, threshold, seed=1234):\n",
    "    random.seed(seed)\n",
    "    # Count the occurrences of each grid_cell\n",
    "    grid_cell_counts = df['grid_cell'].value_counts()\n",
    "\n",
    "    # Sort the grid_cells by count in ascending order\n",
    "    sorted_grid_cells = grid_cell_counts.sort_values()\n",
    "\n",
    "    # Find the threshold index\n",
    "    threshold_index = find_threshold_index(sorted_grid_cells, threshold)\n",
    "    print(f'threshold_index is : {threshold_index}')\n",
    "    \n",
    "    # Group the DataFrame by 'grid_cell'\n",
    "    grouped_df = df.groupby('grid_cell')\n",
    "\n",
    "    accumulated_indices = []\n",
    "    accumulated_count = 0\n",
    "    all_remainings_indices = []\n",
    "\n",
    "    # Iterate over sorted grid_cells and accumulate indices\n",
    "    for grid_cell in sorted_grid_cells.index:\n",
    "        group_indices = grouped_df.get_group(grid_cell).index.tolist()\n",
    "        if len(group_indices) < threshold_index:\n",
    "            accumulated_indices.extend(group_indices)\n",
    "            accumulated_count += len(group_indices)\n",
    "        elif len(group_indices) == threshold_index:\n",
    "            all_remainings_indices.extend(group_indices)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "    # Calculate how many more indices we need to reach the threshold\n",
    "    remaining_count = threshold - accumulated_count\n",
    "    print(f'remaining is : {remaining_count}')\n",
    "\n",
    "    # Randomly select the remaining indices from the current group\n",
    "    accumulated_indices.extend(random.sample(all_remainings_indices, remaining_count))\n",
    "    \n",
    "    return accumulated_indices\n",
    "\n",
    "def generate_cubic(adata, size, seed = 1234):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    data_standardized = scaler.fit_transform(adata.X)\n",
    "    X = data_standardized\n",
    "\n",
    "\n",
    "    random.seed(seed)\n",
    "    print(f'********* #Start# *********')\n",
    "    start_time = time.time()\n",
    "\n",
    "    N_components=adata.shape[1]\n",
    "    pca = PCA(n_components=N_components)\n",
    "    pca.fit(X)\n",
    "    X_pca = pca.transform(X)\n",
    "    df = pd.DataFrame(X_pca[:, :N_components], columns=[f'PC{i+1}' for i in range(N_components)])\n",
    "    df['Label'] = list(adata.obs[label_key].values)\n",
    "\n",
    "\n",
    "    feature_importances = set_min_to_two(pca)\n",
    "\n",
    "    pca_bin_sample_(df, feature_importances)\n",
    "\n",
    "    threshold = size  # Set your desired threshold\n",
    "    samples = accumulate_indices_until_threshold(df, threshold, seed=seed)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "\n",
    "        \n",
    "    return samples, elapsed_time\n",
    "\n",
    "\n",
    "\n",
    "def generate_geo(adata, size, seed=1234):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    data_standardized = scaler.fit_transform(adata.X)\n",
    "\n",
    "    print(f'********* #Start# *********')\n",
    "    np.random.seed(seed)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Compute PCs.\n",
    "    from fbpca import pca\n",
    "    k = adata.shape[1]\n",
    "    U, s, Vt = pca(data_standardized, k=k) # E.g., 4 PCs.\n",
    "    X_dimred = U[:, :k] * s[:k]\n",
    "    # Now, you are ready to sketch!\n",
    "\n",
    "    # Sketch.\n",
    "    from geosketch import gs\n",
    "    N = size # Number of samples to obtain from the data set.\n",
    "    samples = gs(X_dimred, N, replace=False)\n",
    "\n",
    "    # X_sketch = X_dimred[sketch_index]\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "        \n",
    "    return samples, elapsed_time\n",
    "\n",
    "import sys\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "def generate_hopper(adata, size, seed=1234):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    data_standardized = scaler.fit_transform(adata.X)\n",
    "\n",
    "    from hopper.treehopper.hoppers import hopper, treehopper, PCATreePartition\n",
    "    print(f'********* #Start# *********')\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    start_time = time.time()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    data_standardized = scaler.fit_transform(adata.X)\n",
    "    #data_standardized = scaler.fit_transform(adata_shuffled.X)\n",
    "    X = data_standardized\n",
    "\n",
    "    N = size # Number of samples to obtain from the data set.\n",
    "    with io.StringIO() as buf, redirect_stdout(buf):\n",
    "        th = treehopper(data_standardized, partition=PCATreePartition, max_partition_size=1000)\n",
    "        sketch = th.hop(size)\n",
    "\n",
    "    samples = th.path[:size]\n",
    "\n",
    "    # X_sketch = X_dimred[sketch_index]\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "\n",
    "        \n",
    "    return samples, elapsed_time\n",
    "\n",
    "\n",
    "def generate_random(adata, size, seed=1234):\n",
    "\n",
    "    print(f'********* #Start# *********')\n",
    "    np.random.seed(seed)\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    samples = np.random.randint(0, adata.shape[0], size=size)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "\n",
    "        \n",
    "    return samples, elapsed_time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Assuming the sampling functions are defined elsewhere and imported\n",
    "# from sampling_functions import generate_cubic, generate_geo, generate_random, generate_hopper\n",
    "\n",
    "def main(ref, method, size, rep, seed):\n",
    "    # Define a dictionary to store the results\n",
    "    results = []\n",
    "    \n",
    "    address = os.path.join(PATH, f\"{ref}/adata.h5ad\")\n",
    "    adata = sc.read_h5ad(address)\n",
    "    adata.obs[label_key] = adata.obs[label_key].astype('category')\n",
    "    adata.var.index = adata.var.index.astype('object')\n",
    "    \n",
    "    method_dict = {\n",
    "    \"cubic\": (generate_cubic, {\"adata\": adata, \"size\": size, \"seed\": seed}),\n",
    "    \"hopper\": (generate_hopper, {\"adata\": adata, \"size\": size, \"seed\": seed}),\n",
    "    \"random\": (generate_random, {\"adata\": adata, \"size\": size, \"seed\": seed}),\n",
    "    }\n",
    "    \n",
    "    if method in method_dict:\n",
    "        func, args = method_dict[method]\n",
    "        results = func(**args)\n",
    "    else:\n",
    "        print(f\"No function associated with {method}\")\n",
    "        return\n",
    "        \n",
    "    output_address = os.path.join(PATH, f\"{ref}/{method}/{size}/{rep}/results.pkl\")\n",
    "    \n",
    "    with open(output_address, 'wb') as handle:\n",
    "        pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser(description=\"Run sampling methods in parallel for a given Reference, Method, Size, Replicate, and Seed.\")\n",
    "#     parser.add_argument(\"--ref\", type=int, required=True, help=\"Reference to process\")\n",
    "#     parser.add_argument(\"--method\", type=str, required=True, help=\"Method to process\")\n",
    "#     parser.add_argument(\"--size\", type=int, required=True, help=\"Size to process\")\n",
    "#     parser.add_argument(\"--rep\", type=int, required=True, help=\"Replicate to process\")\n",
    "#     parser.add_argument(\"--seed\", type=int, required=True, help=\"Seed to process\")\n",
    "\n",
    "#     args = parser.parse_args()\n",
    "#     print(\"###############################\")\n",
    "#     print(\"************ New run **********\")\n",
    "    \n",
    "    ref = 1\n",
    "    method = 'cubic'\n",
    "    size = 50000\n",
    "    rep = 0\n",
    "    seed = 6547\n",
    "    \n",
    "    main(ref, method, size, rep, seed)\n",
    "    # main(args.ref, args.method, args.size, args.rep, args.seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
