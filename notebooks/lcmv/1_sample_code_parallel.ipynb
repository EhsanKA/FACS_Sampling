{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19bd0387-fb9f-4398-ae61-67f202c3a2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# Get the file path from environment variable\n",
    "file_path_env = '/fast/AG_Ohler/ekarimi/projects/FACS_Sampling/data'\n",
    "OBS_FEATURES = ['prediction','organ','sample_group','label','group','celltype','sample_id']\n",
    "DROP_FEATURES = ['SSC-B-H','SSC-B-A']\n",
    "\n",
    "REFERENCES = [1, 5, 10, 20, 34]\n",
    "METHODS = ['random', 'cubic', 'atomic', 'hopper']\n",
    "methods = ['random', 'cubic', 'hopper']\n",
    "SIZES = [50000, 100000, 200000]\n",
    "REPS = [i for i in range(5)]\n",
    "label_key = 'celltype'\n",
    "\n",
    "\n",
    "directory = \"lcmv/benchmark\"\n",
    "PATH = os.path.join(file_path_env, directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88e91b3f-bb7a-4b64-9f8c-4a11529d2739",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = 10\n",
    "method = 'cubic'\n",
    "size = 50000\n",
    "rep = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d71ba6-553f-4832-ab16-8e4bb9a0e76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "address = os.path.join(PATH, f\"{ref}/adata.h5ad\")\n",
    "\n",
    "adata = sc.read_h5ad(address)\n",
    "\n",
    "adata.obs[label_key] = adata.obs[label_key].astype('category')\n",
    "adata.var.index = adata.var.index.astype('object')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9f775e-9420-4036-ac16-061400b99246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def pca_bin_sample_(df, feature_importances, seed=12345):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Ensure num_pcs does not exceed the number of columns in df\n",
    "    num_pcs = min(feature_importances.shape[0], df.shape[1])\n",
    "\n",
    "    # Function to create bins and digitize\n",
    "    def create_bins_and_digitize(data, n_bins):\n",
    "        edges = np.linspace(data.min(), data.max(), n_bins + 1)\n",
    "        bins = np.digitize(data, edges)\n",
    "        return bins\n",
    "\n",
    "    def compute_sample_bins(df, bin_sizes):\n",
    "        bins = [create_bins_and_digitize(df.iloc[:, i], bin_sizes[i]) for i in range(num_pcs)]\n",
    "\n",
    "        # Combine bins to form grid cells\n",
    "        df['grid_cell'] = list(zip(*bins))\n",
    "        \n",
    "        return \n",
    "\n",
    "    compute_sample_bins(df, feature_importances)\n",
    "    return\n",
    "\n",
    "    \n",
    "def set_min_to_two(pca):\n",
    "    out = np.ceil(pca.explained_variance_ratio_*100).astype(int)\n",
    "    return out[out>2]\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "def find_threshold_index(sorted_grid_cells, threshold):\n",
    "    cumulative = 0\n",
    "    for index, frequency in sorted_grid_cells.value_counts().sort_index().items():\n",
    "        cumulative += index * frequency\n",
    "        if cumulative >= threshold:\n",
    "            return index\n",
    "    return None\n",
    "\n",
    "def accumulate_indices_until_threshold(df, threshold, seed=1234):\n",
    "    random.seed(seed)\n",
    "    # Count the occurrences of each grid_cell\n",
    "    grid_cell_counts = df['grid_cell'].value_counts()\n",
    "\n",
    "    # Sort the grid_cells by count in ascending order\n",
    "    sorted_grid_cells = grid_cell_counts.sort_values()\n",
    "\n",
    "    # Find the threshold index\n",
    "    threshold_index = find_threshold_index(sorted_grid_cells, threshold)\n",
    "    print(f'threshold_index is : {threshold_index}')\n",
    "    \n",
    "    # Group the DataFrame by 'grid_cell'\n",
    "    grouped_df = df.groupby('grid_cell')\n",
    "\n",
    "    accumulated_indices = []\n",
    "    accumulated_count = 0\n",
    "    all_remainings_indices = []\n",
    "\n",
    "    # Iterate over sorted grid_cells and accumulate indices\n",
    "    for grid_cell in sorted_grid_cells.index:\n",
    "        group_indices = grouped_df.get_group(grid_cell).index.tolist()\n",
    "        if len(group_indices) < threshold_index:\n",
    "            accumulated_indices.extend(group_indices)\n",
    "            accumulated_count += len(group_indices)\n",
    "        elif len(group_indices) == threshold_index:\n",
    "            all_remainings_indices.extend(group_indices)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "    # Calculate how many more indices we need to reach the threshold\n",
    "    remaining_count = threshold - accumulated_count\n",
    "    print(f'remaining is : {remaining_count}')\n",
    "\n",
    "    # Randomly select the remaining indices from the current group\n",
    "    accumulated_indices.extend(random.sample(all_remainings_indices, remaining_count))\n",
    "    \n",
    "    return accumulated_indices\n",
    "\n",
    "def generate_cubic(adata, rep=1, size=10000, seed = 1234):\n",
    "    scaler = StandardScaler()\n",
    "    data_standardized = scaler.fit_transform(adata.X)\n",
    "    #data_standardized = scaler.fit_transform(adata_shuffled.X)\n",
    "    X = data_standardized\n",
    "\n",
    "    cubic_time= []\n",
    "    cubic_samples = []\n",
    "    for i in range(rep):\n",
    "        random.seed(seed+i)\n",
    "        print(f'********* #full dataset *********')\n",
    "        start_time = time.time()\n",
    "        \n",
    "        N_components=adata.shape[1]\n",
    "        pca = PCA(n_components=N_components)\n",
    "        pca.fit(X)\n",
    "        X_pca = pca.transform(X)\n",
    "        df = pd.DataFrame(X_pca[:, :N_components], columns=[f'PC{i+1}' for i in range(N_components)])\n",
    "        df['Label'] = list(adata.obs[label_key].values)\n",
    "\n",
    "\n",
    "        feature_importances = set_min_to_two(pca)\n",
    "\n",
    "        pca_bin_sample_(df, feature_importances)\n",
    "\n",
    "        threshold = size  # Set your desired threshold\n",
    "        accumulated_indices = accumulate_indices_until_threshold(df, threshold, seed=seed+i)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "        cubic_time.append(elapsed_time)\n",
    "        cubic_samples.append(accumulated_indices)\n",
    "        \n",
    "    return cubic_samples, cubic_time\n",
    "\n",
    "def generate_geo(adata, rep=1, size=10000, seed=1234):\n",
    "    scaler = StandardScaler()\n",
    "    data_standardized = scaler.fit_transform(adata.X)\n",
    "    #data_standardized = scaler.fit_transform(adata_shuffled.X)\n",
    "    X = data_standardized\n",
    "    geo_time= []\n",
    "    geo_samples = []\n",
    "    for i in range(rep):\n",
    "        np.random.seed(seed+i)\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Compute PCs.\n",
    "        from fbpca import pca\n",
    "        k = adata.shape[1]\n",
    "        U, s, Vt = pca(X, k=k) # E.g., 4 PCs.\n",
    "        X_dimred = U[:, :k] * s[:k]\n",
    "        # Now, you are ready to sketch!\n",
    "\n",
    "        # Sketch.\n",
    "        from geosketch import gs\n",
    "        N = size # Number of samples to obtain from the data set.\n",
    "        sketch_index = gs(X_dimred, N, replace=False)\n",
    "\n",
    "        # X_sketch = X_dimred[sketch_index]\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "        geo_time.append(elapsed_time)\n",
    "        geo_samples.append(sketch_index)\n",
    "        \n",
    "    return geo_samples, geo_time\n",
    "\n",
    "import sys\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "def generate_hopper(adata, rep=1, size=10000, seed=1234):\n",
    "    scaler = StandardScaler()\n",
    "    data_standardized = scaler.fit_transform(adata.X)\n",
    "    #data_standardized = scaler.fit_transform(adata_shuffled.X)\n",
    "    X = data_standardized\n",
    "    hopper_time= []\n",
    "    hopper_samples = []\n",
    "    for i in range(rep):\n",
    "        from hopper.treehopper.hoppers import hopper, treehopper, PCATreePartition\n",
    "        \n",
    "        np.random.seed(seed+i)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        data_standardized = scaler.fit_transform(adata.X)\n",
    "        #data_standardized = scaler.fit_transform(adata_shuffled.X)\n",
    "        X = data_standardized\n",
    "        \n",
    "        N = size # Number of samples to obtain from the data set.\n",
    "        with io.StringIO() as buf, redirect_stdout(buf):\n",
    "            th = treehopper(X, partition=PCATreePartition, max_partition_size=1000, )\n",
    "            sketch = th.hop(size)\n",
    "            \n",
    "        sketch_index = th.path[:size]\n",
    "\n",
    "        # X_sketch = X_dimred[sketch_index]\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "        hopper_time.append(elapsed_time)\n",
    "        hopper_samples.append(sketch_index)\n",
    "        \n",
    "    return hopper_samples, hopper_time\n",
    "\n",
    "\n",
    "def generate_random(adata, rep=1, size=10000, seed=1234):\n",
    "    random_time= []\n",
    "    random_samples = []\n",
    "    for i in range(rep):\n",
    "        print(f'********* #full dataset *********')\n",
    "        np.random.seed(seed+i)\n",
    "        start_time = time.time()\n",
    "\n",
    "\n",
    "        # threshold = 27446  # Set your desired threshold\n",
    "        rs = np.random.randint(0, adata.shape[0], size=size)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "\n",
    "        random_time.append(elapsed_time)\n",
    "        random_samples.append(rs)\n",
    "        \n",
    "    return random_samples, random_time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05c610ff-95c2-42c6-a73e-3a5440d4c6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def pca_bin_sample_(df, feature_importances, seed=12345):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Ensure num_pcs does not exceed the number of columns in df\n",
    "    num_pcs = min(feature_importances.shape[0], df.shape[1])\n",
    "\n",
    "    # Function to create bins and digitize\n",
    "    def create_bins_and_digitize(data, n_bins):\n",
    "        edges = np.linspace(data.min(), data.max(), n_bins + 1)\n",
    "        bins = np.digitize(data, edges)\n",
    "        return bins\n",
    "\n",
    "    def compute_sample_bins(df, bin_sizes):\n",
    "        bins = [create_bins_and_digitize(df.iloc[:, i], bin_sizes[i]) for i in range(num_pcs)]\n",
    "\n",
    "        # Combine bins to form grid cells\n",
    "        df['grid_cell'] = list(zip(*bins))\n",
    "        \n",
    "        return \n",
    "\n",
    "    compute_sample_bins(df, feature_importances)\n",
    "    return\n",
    "\n",
    "    \n",
    "def set_min_to_two(pca):\n",
    "    out = np.ceil(pca.explained_variance_ratio_*100).astype(int)\n",
    "    return out[out>2]\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "def find_threshold_index(sorted_grid_cells, threshold):\n",
    "    cumulative = 0\n",
    "    for index, frequency in sorted_grid_cells.value_counts().sort_index().items():\n",
    "        cumulative += index * frequency\n",
    "        if cumulative >= threshold:\n",
    "            return index\n",
    "    return None\n",
    "\n",
    "def accumulate_indices_until_threshold(df, threshold, seed=1234):\n",
    "    random.seed(seed)\n",
    "    # Count the occurrences of each grid_cell\n",
    "    grid_cell_counts = df['grid_cell'].value_counts()\n",
    "\n",
    "    # Sort the grid_cells by count in ascending order\n",
    "    sorted_grid_cells = grid_cell_counts.sort_values()\n",
    "\n",
    "    # Find the threshold index\n",
    "    threshold_index = find_threshold_index(sorted_grid_cells, threshold)\n",
    "    print(f'threshold_index is : {threshold_index}')\n",
    "    \n",
    "    # Group the DataFrame by 'grid_cell'\n",
    "    grouped_df = df.groupby('grid_cell')\n",
    "\n",
    "    accumulated_indices = []\n",
    "    accumulated_count = 0\n",
    "    all_remainings_indices = []\n",
    "\n",
    "    # Iterate over sorted grid_cells and accumulate indices\n",
    "    for grid_cell in sorted_grid_cells.index:\n",
    "        group_indices = grouped_df.get_group(grid_cell).index.tolist()\n",
    "        if len(group_indices) < threshold_index:\n",
    "            accumulated_indices.extend(group_indices)\n",
    "            accumulated_count += len(group_indices)\n",
    "        elif len(group_indices) == threshold_index:\n",
    "            all_remainings_indices.extend(group_indices)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "    # Calculate how many more indices we need to reach the threshold\n",
    "    remaining_count = threshold - accumulated_count\n",
    "    print(f'remaining is : {remaining_count}')\n",
    "\n",
    "    # Randomly select the remaining indices from the current group\n",
    "    accumulated_indices.extend(random.sample(all_remainings_indices, remaining_count))\n",
    "    \n",
    "    return accumulated_indices\n",
    "\n",
    "def generate_cubic(adata, size, seed = 1234):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    data_standardized = scaler.fit_transform(adata.X)\n",
    "    X = data_standardized\n",
    "\n",
    "\n",
    "    random.seed(seed)\n",
    "    print(f'********* #Start# *********')\n",
    "    start_time = time.time()\n",
    "\n",
    "    N_components=adata.shape[1]\n",
    "    pca = PCA(n_components=N_components)\n",
    "    pca.fit(X)\n",
    "    X_pca = pca.transform(X)\n",
    "    df = pd.DataFrame(X_pca[:, :N_components], columns=[f'PC{i+1}' for i in range(N_components)])\n",
    "    df['Label'] = list(adata.obs[label_key].values)\n",
    "\n",
    "\n",
    "    feature_importances = set_min_to_two(pca)\n",
    "\n",
    "    pca_bin_sample_(df, feature_importances)\n",
    "\n",
    "    threshold = size  # Set your desired threshold\n",
    "    samples = accumulate_indices_until_threshold(df, threshold, seed=seed)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "\n",
    "        \n",
    "    return samples, elapsed_time\n",
    "\n",
    "def generate_geo(adata, size, seed=1234):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    data_standardized = scaler.fit_transform(adata.X)\n",
    "    X = data_standardized\n",
    "\n",
    "    print(f'********* #Start# *********')\n",
    "    np.random.seed(seed)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Compute PCs.\n",
    "    from fbpca import pca\n",
    "    k = adata.shape[1]\n",
    "    U, s, Vt = pca(X, k=k) # E.g., 4 PCs.\n",
    "    X_dimred = U[:, :k] * s[:k]\n",
    "    # Now, you are ready to sketch!\n",
    "\n",
    "    # Sketch.\n",
    "    from geosketch import gs\n",
    "    N = size # Number of samples to obtain from the data set.\n",
    "    samples = gs(X_dimred, N, replace=False)\n",
    "\n",
    "    # X_sketch = X_dimred[sketch_index]\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "        \n",
    "    return samples, elapsed_time\n",
    "\n",
    "import sys\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "def generate_hopper(adata, size, seed=1234):\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    data_standardized = scaler.fit_transform(adata.X)\n",
    "    X = data_standardized\n",
    "\n",
    "    from hopper.treehopper.hoppers import hopper, treehopper, PCATreePartition\n",
    "    print(f'********* #Start# *********')\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    start_time = time.time()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    data_standardized = scaler.fit_transform(adata.X)\n",
    "    #data_standardized = scaler.fit_transform(adata_shuffled.X)\n",
    "    X = data_standardized\n",
    "\n",
    "    N = size # Number of samples to obtain from the data set.\n",
    "    with io.StringIO() as buf, redirect_stdout(buf):\n",
    "        th = treehopper(X, partition=PCATreePartition, max_partition_size=1000)\n",
    "        sketch = th.hop(size)\n",
    "\n",
    "    samples = th.path[:size]\n",
    "\n",
    "    # X_sketch = X_dimred[sketch_index]\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "\n",
    "        \n",
    "    return samples, elapsed_time\n",
    "\n",
    "\n",
    "def generate_random(adata, size, seed=1234):\n",
    "\n",
    "    print(f'********* #Start# *********')\n",
    "    np.random.seed(seed)\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    samples = np.random.randint(0, adata.shape[0], size=size)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "\n",
    "        \n",
    "    return samples, elapsed_time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73c68f22-34b2-4782-b66a-2e390b70caf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********* #Start# *********\n",
      "threshold_index is : 1\n",
      "remaining is : 50000\n",
      "Elapsed time: 63.4642550945282 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "\n",
    "results = {}\n",
    "\n",
    "results = generate_cubic(adata, size=50000, seed=1456)\n",
    "    # results['geo'][f'{batch_id}_{size}'] = generate_geo(batched_adatas[batch_id], size=size, rep=rep)\n",
    "    # results['random'][f'{batch_id}_{size}'] = generate_random(batched_adatas[batch_id], size=size, rep=rep)\n",
    "    # results['hopper'][f'{batch_id}_{size}'] = generate_hopper(batched_adatas[batch_id], size=size, rep=rep)\n",
    "    # Save the results to disk for later retrieval\n",
    "# with open(f'sampling_results_{batch_id}.pkl', 'wb') as handle:\n",
    "#     pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b206079e-63d0-4e75-87d7-73967dc497b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "REFERENCES = [1, 5, 10, 20, 34]\n",
    "METHODS = ['random', 'cubic', 'atomic', 'hopper']\n",
    "methods = ['random', 'cubic', 'hopper']\n",
    "SIZES = [50000, 100000, 200000]\n",
    "REPS = [i for i in range(5)]\n",
    "label_key = 'celltype'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "734e0bb5-b3d1-4b8e-9040-531958924ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Assuming the sampling functions are defined elsewhere and imported\n",
    "# from sampling_functions import generate_cubic, generate_geo, generate_random, generate_hopper\n",
    "\n",
    "def main(ref, method, size, rep, seed):\n",
    "    # Define a dictionary to store the results\n",
    "    results = []\n",
    "    \n",
    "    address = os.path.join(PATH, f\"{ref}/adata.h5ad\")\n",
    "    adata = sc.read_h5ad(address)\n",
    "    adata.obs[label_key] = adata.obs[label_key].astype('category')\n",
    "    adata.var.index = adata.var.index.astype('object')\n",
    "    \n",
    "    method_dict = {\n",
    "    \"cubic\": (generate_cubic, {\"adata\": adata, \"size\": size, \"seed\": seed}),\n",
    "    \"hopper\": (generate_hopper, {\"adata\": adata, \"size\": size, \"seed\": seed}),\n",
    "    \"random\": (generate_random, {\"adata\": adata, \"size\": size, \"seed\": seed}),\n",
    "    }\n",
    "    \n",
    "    if method in method_dict:\n",
    "        func, args = method_dict[method]\n",
    "        results = func(**args)\n",
    "    else:\n",
    "        print(f\"No function associated with {method}\")\n",
    "        return\n",
    "        \n",
    "    output_address = os.path.join(PATH, f\"{ref}/{method}/{size}/{rep}/results.pkl\")\n",
    "    \n",
    "    with open(output_address, 'wb') as handle:\n",
    "        pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1ebdb478-e108-4e5b-8101-d48078cb9af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################\n",
      "************ New run **********\n",
      "********* #Start# *********\n",
      "threshold_index is : 3\n",
      "remaining is : 2286\n",
      "Elapsed time: 9.629080533981323 seconds\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser(description=\"Run sampling methods in parallel for a given Reference, Method, Size, Replicate, and Seed.\")\n",
    "#     parser.add_argument(\"--ref\", type=int, required=True, help=\"Reference to process\")\n",
    "#     parser.add_argument(\"--method\", type=str, required=True, help=\"Method to process\")\n",
    "#     parser.add_argument(\"--size\", type=int, required=True, help=\"Size to process\")\n",
    "#     parser.add_argument(\"--rep\", type=int, required=True, help=\"Replicate to process\")\n",
    "#     parser.add_argument(\"--seed\", type=int, required=True, help=\"Seed to process\")\n",
    "\n",
    "#     args = parser.parse_args()\n",
    "    print(\"###############################\")\n",
    "    print(\"************ New run **********\")\n",
    "    \n",
    "    ref = 1\n",
    "    method = 'cubic'\n",
    "    size = 50000\n",
    "    rep = 0\n",
    "    seed = 6547\n",
    "    \n",
    "    # main(args.batch_id, args.size)\n",
    "    main(ref, method, size, rep, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910d3e3e-20b1-4f01-b513-243a8cc98826",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Run sampling methods in parallel for a given Reference, Method, Size, Replicate, and Seed.\")\n",
    "    parser.add_argument(\"--ref\", type=int, required=True, help=\"Reference to process\")\n",
    "    parser.add_argument(\"--method\", type=str, required=True, help=\"Method to process\")\n",
    "    parser.add_argument(\"--size\", type=int, required=True, help=\"Size to process\")\n",
    "    parser.add_argument(\"--rep\", type=int, required=True, help=\"Replicate to process\")\n",
    "    parser.add_argument(\"--seed\", type=int, required=True, help=\"Seed to process\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    print(\"###############################\")\n",
    "    print(\"************ New run **********\")\n",
    "    \n",
    "    # ref = 1\n",
    "    # method = 'cubic'\n",
    "    # size = 50000\n",
    "    # rep = 0\n",
    "    # seed = 6547\n",
    "    \n",
    "    # main(args.batch_id, args.size)\n",
    "    # main(ref, method, size, rep, seed)\n",
    "    main(args.ref, args.method, args.size, args.rep, args.seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:facs_sampling]",
   "language": "python",
   "name": "conda-env-facs_sampling-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
